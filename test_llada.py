#!/usr/bin/env python3
"""
Test script for LLaDA model implementation in mlx-lm
"""

import sys
import traceback
from mlx_lm import load, generate

def test_llada_model():
    """Test the LLaDA model loading and generation"""
    
    print("ğŸš€ Testing LLaDA model implementation...")
    print("-" * 50)
    
    try:
        # Load the model
        print("ğŸ“¥ Loading LLaDA model...")
        model, tokenizer = load("mlx-community/LLaDA-8B-Instruct-mlx-4bit")
        print("âœ… Model loaded successfully!")
        
        # Print model info
        print(f"ğŸ“Š Model type: {type(model)}")
        print(f"ğŸ“Š Model args: {model.args}")
        print(f"ğŸ“Š Vocab size: {model.args.vocab_size}")
        print(f"ğŸ“Š Hidden size: {model.args.hidden_size}")
        print(f"ğŸ“Š Num layers: {model.args.num_hidden_layers}")
        
        # Test basic generation
        print("\nğŸ§ª Testing basic generation...")
        prompt = "Hello, how are you?"
        
        # Apply chat template if available
        if tokenizer.chat_template is not None:
            messages = [{"role": "user", "content": prompt}]
            formatted_prompt = tokenizer.apply_chat_template(
                messages, add_generation_prompt=True
            )
            print(f"ğŸ’¬ Using chat template: {formatted_prompt[:100]}...")
        else:
            formatted_prompt = prompt
            print(f"ğŸ’¬ Using raw prompt: {formatted_prompt}")
        
        # Generate response
        print("ğŸ¯ Generating response...")
        response = generate(
            model, 
            tokenizer, 
            prompt=formatted_prompt, 
            verbose=True, 
            max_tokens=50,
            temp=0.7
        )
        
        print(f"\nğŸ‰ Generated response:")
        print(f"ğŸ“ {response}")
        
        # Test with a more complex prompt
        print("\nğŸ§ª Testing with complex prompt...")
        complex_prompt = "Explain the concept of artificial intelligence in simple terms."
        
        if tokenizer.chat_template is not None:
            messages = [{"role": "user", "content": complex_prompt}]
            formatted_complex_prompt = tokenizer.apply_chat_template(
                messages, add_generation_prompt=True
            )
        else:
            formatted_complex_prompt = complex_prompt
            
        complex_response = generate(
            model, 
            tokenizer, 
            prompt=formatted_complex_prompt, 
            verbose=True, 
            max_tokens=100,
            temp=0.7
        )
        
        print(f"\nğŸ‰ Complex response:")
        print(f"ğŸ“ {complex_response}")
        
        print("\nâœ… All tests passed! LLaDA implementation is working correctly.")
        return True
        
    except Exception as e:
        print(f"\nâŒ Error occurred: {e}")
        print("\nğŸ” Full traceback:")
        traceback.print_exc()
        return False

def test_model_architecture():
    """Test that the model architecture is correct"""
    
    print("\nğŸ—ï¸  Testing model architecture...")
    
    try:
        from mlx_lm.models.llada import ModelArgs, Model
        
        # Test ModelArgs creation
        config = {
            "model_type": "llada",
            "d_model": 4096,
            "n_layers": 32,
            "mlp_hidden_size": 12288,
            "n_heads": 32,
            "vocab_size": 126464,
            "rms_norm_eps": 1e-05,
        }
        
        args = ModelArgs.from_dict(config)
        print(f"âœ… ModelArgs created: {args}")
        
        # Test Model creation
        model = Model(args)
        print(f"âœ… Model created successfully")
        print(f"ğŸ“Š Model layers: {len(model.layers)}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Architecture test failed: {e}")
        traceback.print_exc()
        return False

def main():
    """Main test function"""
    
    print("ğŸ”¬ LLaDA Model Test Suite")
    print("=" * 50)
    
    # Test architecture first
    arch_success = test_model_architecture()
    
    if not arch_success:
        print("âŒ Architecture tests failed, skipping model loading tests")
        sys.exit(1)
    
    # Test full model loading and generation
    model_success = test_llada_model()
    
    if model_success:
        print("\nğŸŠ All tests completed successfully!")
        print("ğŸš€ LLaDA model is ready for use!")
        sys.exit(0)
    else:
        print("\nğŸ’¥ Some tests failed!")
        sys.exit(1)

if __name__ == "__main__":
    main()